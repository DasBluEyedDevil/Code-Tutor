---
phase: 07-python-course-audit
plan: 04
type: execute
wave: 2
depends_on:
  - 07-02
  - 07-03
files_modified:
  - content/courses/python/modules/*/lessons/*/challenge.json
  - content/courses/python/modules/*/lessons/*/starter.py
  - content/courses/python/modules/*/lessons/*/solution.py
autonomous: true

must_haves:
  truths:
    - Every coding challenge solution executes successfully
    - Every coding challenge test validation passes
    - Broken challenges are fixed inline immediately
    - Challenge instructions match the solution requirements
    - Capstone project builds and runs correctly
  artifacts:
    - path: "content/courses/python/modules/*/lessons/*/challenge.json"
      provides: "Valid challenge configurations with correct test cases"
    - path: "content/courses/python/modules/*/lessons/*/solution.py"
      provides: "Working solution code that passes all tests"
    - path: "content/courses/python/modules/*/lessons/*/starter.py"
      provides: "Starter code that can be completed to pass tests"
  key_links:
    - from: "challenge.json test cases"
      to: "solution.py"
      via: "execution validation"
      pattern: "solution passes all test cases"
    - from: "starter.py"
      to: "solution.py"
      via: "completion path"
      pattern: "starter can be completed to match solution"
---

<objective>
Validate all Python coding challenges by executing every solution against its test cases. Fix broken challenges inline immediately. Ensure the capstone project builds and runs correctly.

Purpose: Satisfy PYTH-03 (all challenges execute and validate correctly) and PYTH-04 (deployable capstone). This follows the challenge validation pattern from Phases 2-6.

Output: All Python challenges verified working, broken challenges fixed, capstone validated.
</objective>

<execution_context>
@~/.config/opencode/get-shit-done/workflows/execute-plan.md
@~/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/REQUIREMENTS.md
@.planning/phases/07-python-course-audit/07-CONTEXT.md
@.planning/phases/07-python-course-audit/07-02-SUMMARY.md
@.planning/phases/07-python-course-audit/07-03-SUMMARY.md

## Challenge Fix Strategy (from 07-CONTEXT.md)

- Broken challenges: Fix inline immediately — pause validation, fix, verify, continue (no batching)
- Solution verification: Run every solution to confirm it passes (100% verification, not spot-checking)
- Fundamentally broken challenges: Replace with a new challenge teaching the same concept (don't remove or convert to quiz)
- Lesson synchronization: Always keep challenges and lessons in sync — challenge fixes trigger lesson content updates when needed

## Prior Phase Challenge Patterns

From STATE.md:
- Phase 2 (Java): All challenges compiled and ran against Java 25
- Phase 3 (JS): Simulation wrappers added for Bun-only challenges, all 304 JSON files validated
- Phase 4 (C#): 3 C# 13 challenges given C# 12 fallback solutions
- Phase 5 (Flutter): 217 challenges validated, 44 missing solutions categorized (QUIZ/MULTI_CHOICE don't need code)
- Phase 6 (Kotlin): 80+ challenges created, pure logic extraction pattern for framework-independent challenges

## Known Issues (from STATE.md)

- Fix malformed challenge.json in Python module 01 lesson 05 (invalid JSON with unescaped quotes in hints)
- Normalize challenge type values across courses (9 different types exist)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Fix known challenge issues and validate JSON</name>
  <files>content/courses/python/modules/01-the-absolute-basics/lessons/05-*/challenge.json</files>
  <action>
    Fix the known malformed challenge.json in Module 01 Lesson 05:
    
    1. Read the challenge.json file
    2. Identify the JSON syntax error (unescaped quotes in hints)
    3. Fix the escaping issues
    4. Validate the JSON is now valid
    
    Then perform global JSON validation:
    - Validate all challenge.json files in Python course
    - Validate all lesson.json files
    - Validate all module.json files
    - Report any JSON syntax errors found
    
    Also check challenge type values:
    - List all unique challenge type values found
    - Note any inconsistencies (FREE_CODING vs coding, QUIZ vs quiz, etc.)
    - This is informational for future normalization
  </action>
  <verify>
    # Validate the fixed challenge
    python -c "import json; json.load(open('content/courses/python/modules/01-the-absolute-basics/lessons/05-*/challenge.json'))"
    
    # Validate all Python course JSON files
    find content/courses/python -name "*.json" -exec python -c "import json; json.load(open('{}'))" \; 2>&1 | head -20
  </verify>
  <done>
    - M01 L05 challenge.json is valid JSON
    - All Python course JSON files validate successfully
    - Challenge type values catalogued
  </done>
</task>

<task type="auto">
  <name>Task 2: Validate challenges - Fundamentals (M01-05)</name>
  <files>content/courses/python/modules/0[1-5]-*/lessons/*/challenge.json, content/courses/python/modules/0[1-5]-*/lessons/*/solution.py</files>
  <action>
    Validate all challenges in Modules 01-05:
    
    For each challenge:
    1. Read challenge.json to understand test cases
    2. Read solution.py
    3. Execute solution.py with Python 3.12+
    4. Verify output matches expected results
    5. If challenge has automated tests, run them
    
    Challenge types:
    - FREE_CODING: Execute solution, verify it runs without errors
    - QUIZ: Verify answer options are correct (no code execution needed)
    - MULTI_CHOICE: Verify answer options are correct
    
    Fix broken challenges inline:
    - If solution has syntax error: Fix the syntax
    - If solution logic is wrong: Fix the logic
    - If test cases are wrong: Fix the test cases
    - If starter.py is missing: Create appropriate starter
    - If challenge is fundamentally broken: Replace with new challenge teaching same concept
    
    Document any lesson content that needs updating due to challenge fixes.
  </action>
  <verify>
    # Count challenges in M01-05
    find content/courses/python/modules/0[1-5]-*/lessons -name "challenge.json" | wc -l
    
    # Test a sample of solutions
    for f in content/courses/python/modules/01-*/lessons/*/solution.py; do
      echo "Testing: $f"
      python "$f" 2>&1 && echo "PASS" || echo "FAIL"
    done | head -40
  </verify>
  <done>
    - All M01-05 challenges validated
    - Broken challenges fixed inline
    - All solutions execute successfully
    - Lesson content updated if needed
  </done>
</task>

<task type="auto">
  <name>Task 3: Validate challenges - Functions through OOP (M06-11)</name>
  <files>content/courses/python/modules/0[6-9]-*/lessons/*/challenge.json, content/courses/python/modules/1[0-1]-*/lessons/*/challenge.json, content/courses/python/modules/0[6-9]-*/lessons/*/solution.py, content/courses/python/modules/1[0-1]-*/lessons/*/solution.py</files>
  <action>
    Validate all challenges in Modules 06-11:
    
    M06: Functions
    - Verify function definition challenges work
    - Check default argument challenges
    - Validate *args, **kwargs challenges
    
    M07: Dictionaries
    - Verify dict manipulation challenges
    - Check dictionary comprehension challenges
    
    M08: Exception Handling
    - Verify try/except challenges
    - Check custom exception challenges
    
    M09: File I/O
    - Verify file reading/writing challenges
    - Check pathlib usage challenges
    - Note: Some file I/O challenges may need temporary file setup
    
    M10: Modules and Packages
    - Verify import challenges
    - Check module structure challenges
    
    M11: OOP
    - Verify class definition challenges
    - Check inheritance challenges
    - Validate method override challenges
    
    Fix broken challenges inline following the same pattern as Task 2.
  </action>
  <verify>
    # Count challenges
    find content/courses/python/modules/0[6-9]-*/lessons -name "challenge.json" | wc -l
    find content/courses/python/modules/1[0-1]-*/lessons -name "challenge.json" | wc -l
    
    # Sample validation
    find content/courses/python/modules/0[6-9]-*/lessons -name "solution.py" | head -5 | while read f; do
      python "$f" 2>&1 && echo "PASS: $f" || echo "FAIL: $f"
    done
  </verify>
  <done>
    - All M06-11 challenges validated
    - Broken challenges fixed inline
    - All solutions execute successfully
  </done>
</task>

<task type="auto">
  <name>Task 4: Validate challenges - Advanced and Web Frameworks (M12-17)</name>
  <files>content/courses/python/modules/1[2-7]-*/lessons/*/challenge.json, content/courses/python/modules/1[2-7]-*/lessons/*/solution.py</files>
  <action>
    Validate all challenges in Modules 12-17:
    
    M12: Decorators and Advanced Patterns
    - Verify decorator challenges
    - Check context manager challenges
    
    M13: Async Python
    - Verify async/await challenges
    - Check asyncio patterns
    - Note: Some async challenges may need special handling (asyncio.run())
    
    M14: FastAPI
    - Verify FastAPI endpoint challenges
    - Check Pydantic model challenges
    - Note: Web framework challenges may need mock/test client approach
    
    M15: SQLAlchemy
    - Verify ORM query challenges
    - Check model definition challenges
    - Note: Database challenges may need in-memory SQLite
    
    M16: API Authentication
    - Verify authentication flow challenges
    - Check JWT handling challenges
    
    M17: Sharing Your Work
    - Verify packaging challenges
    - Check may be more conceptual (QUIZ type)
    
    Special handling for framework-dependent challenges:
    - If challenge requires FastAPI/SQLAlchemy imports, ensure they're available
    - Use test clients or mocks where appropriate
    - For database challenges, use :memory: SQLite
    
    Fix broken challenges inline.
  </action>
  <verify>
    # Count challenges
    find content/courses/python/modules/1[2-7]-*/lessons -name "challenge.json" | wc -l
    
    # Check for framework import issues
    grep -r "from fastapi import\|from sqlalchemy import" content/courses/python/modules/1[4-5]-*/lessons/*/solution.py | head -10
  </verify>
  <done>
    - All M12-17 challenges validated
    - Framework-dependent challenges handled appropriately
    - Broken challenges fixed inline
  </done>
</task>

<task type="auto">
  <name>Task 5: Validate challenges - Advanced Topics and Capstone (M18-24)</name>
  <files>content/courses/python/modules/1[8-9]-*/lessons/*/challenge.json, content/courses/python/modules/2[0-4]-*/lessons/*/challenge.json, content/courses/python/modules/1[8-9]-*/lessons/*/solution.py, content/courses/python/modules/2[0-4]-*/lessons/*/solution.py</files>
  <action>
    Validate all challenges in Modules 18-24:
    
    M18: Typer CLI
    - Verify CLI argument handling challenges
    
    M19: Exception Groups
    - Verify ExceptionGroup handling challenges
    - Check TaskGroup usage challenges
    
    M20: pytest
    - Verify test writing challenges
    - Check fixture and parametrize challenges
    
    M21: Django
    - Verify Django model/view challenges
    - Check may need Django test runner approach
    
    M22: PostgreSQL Advanced
    - Verify advanced database pattern challenges
    
    M23: Authentication and Security
    - Verify security implementation challenges
    
    M24: Capstone
    - Verify capstone has clear build instructions
    - Check all dependencies are specified
    - Validate the project can be run locally
    - Verify deployment instructions are complete
    
    Capstone validation:
    1. Read capstone lesson content
    2. Check for requirements.txt or pyproject.toml
    3. Verify all code examples are complete
    4. Ensure deployment instructions are clear
    5. Document any setup requirements
    
    Fix broken challenges inline.
  </action>
  <verify>
    # Count challenges
    find content/courses/python/modules/1[8-9]-*/lessons -name "challenge.json" | wc -l
    find content/courses/python/modules/2[0-4]-*/lessons -name "challenge.json" | wc -l
    
    # Check capstone structure
    ls -la content/courses/python/modules/24-*/
    find content/courses/python/modules/24-*/lessons -name "*.md" | head -5
  </verify>
  <done>
    - All M18-24 challenges validated
    - Capstone project is complete and deployable
    - All solutions execute successfully
    - PYTH-03 and PYTH-04 satisfied
  </done>
</task>

</tasks>

<verification>
1. All challenge.json files are valid JSON
2. All challenges in M01-24 have been validated
3. All solution.py files execute successfully
4. Broken challenges have been fixed inline
5. Capstone project (M24) is complete and deployable
6. PYTH-03 (all challenges execute correctly) is satisfied
7. PYTH-04 (deployable capstone) is satisfied
</verification>

<success_criteria>
- Every coding challenge solution executes successfully against Python 3.12+
- Every challenge test validation passes
- All broken challenges have been fixed inline
- Challenge instructions match solution requirements
- Capstone project (Personal Finance Tracker) builds and runs correctly
- All JSON files are valid
- PYTH-03 and PYTH-04 requirements satisfied
</success_criteria>

<output>
After completion, create `.planning/phases/07-python-course-audit/07-04-SUMMARY.md`
</output>
